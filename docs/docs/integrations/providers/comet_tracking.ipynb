{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comet Opik\n",
    "\n",
    "Opik is an open source end-to-end [LLM Evaluation Platform](https://www.comet.com/site/products/opik/?utm_source=langchain&utm_medium=docs&utm_content=provider_intro_paragraph&utm_campaign=opik) that helps developers track their LLM prompts and responses during both development and production. Users can define and run evaluations to test their LLMs apps before deployment to check for hallucinations, accuracy, context retrieval, and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/comet-ml/opik/refs/heads/main/apps/opik-documentation/documentation/static/img/opik-logo.svg\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the colab to see an end to end example on how you can leverage [Opik](https://www.comet.com/site/products/opik/?utm_source=langchain&utm_medium=docs&utm_content=second_paragraph&utm_campaign=opik) to track and evaluate your LLM Applications you are building with Langchain!\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/langchain.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Configure Opik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to get started with Opik! \n",
    "\n",
    "1. Sign up for a SaaS Version and start logging with your [Opik API Key](https://www.comet.com/signup?from=llm&utm_source=langchain&utm_medium=docs&utm_content=api_key&utm_campaign=opik)\n",
    "\n",
    "\n",
    "2. Self-Host Opik following the [Installation Instructions](https://github.com/comet-ml/opik?tab=readme-ov-file#%EF%B8%8F-installation) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opik\n",
    "\n",
    "opik.configure(use_local=False) #set to True if you choose to self-host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpikTracer \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Method Example\n",
    "\n",
    "Here is a basic example of how to use the `OpikTracer` callback with a LangChain chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "# Initialize the tracer\n",
    "opik_tracer = OpikTracer()\n",
    "\n",
    "# Create the LLM Chain using LangChain\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"Translate the following text to French: {input}\"\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Generate the translations\n",
    "translation = llm_chain.run(\"Hello, how are you?\", callbacks=[opik_tracer])\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The  `OpikTracer` will automatically log the input prompt, the output, and any metadata for each step in the chain\n",
    "\n",
    "\n",
    "![opik_langchain](docs/static/img/opik_langchain_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke Example\n",
    "If you use the `invoke` method when running your Langchain Chains, you can call the `OpikTracer` with the `with_config` method as shown in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "model = ChatOpenAI(model='gpt-4')\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\")\n",
    "\n",
    "opik_tracer = OpikTracer()\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.with_config(callbacks=[opik_tracer]).invoke({\"topic\": \"bears\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Metadata and Tags \n",
    "\n",
    "Log additional metadata or tags to your Langchain Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "opik_tracer = OpikTracer(\n",
    "    tags=[\"langchain\"],\n",
    "    metadata={\"use-case\": \"documentation-example\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Logged Traces\n",
    "\n",
    "You can retrieve Traces logged to Opik with the `created_traces`  method. This can be helpful if you need to log feedback scores or update a trace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik.integrations.langchain import OpikTracer\n",
    "\n",
    "opik_tracer = OpikTracer()\n",
    "\n",
    "# Calling Langchain object\n",
    "\n",
    "traces = opik_tracer.created_traces()\n",
    "for trace in traces:\n",
    "    trace.log_feedback_score('your_score_name', your_scoring_function())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Evaluations with Opik \n",
    "\n",
    "Opik also has the ability for use to define a dataset and run automated evaluations on their to test and score their model responses. Opik provides a number of out-of-the-box metrics to score your LLM responses but also gives users the flexibility to define their own! Here is a [end-to-end colab](https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/langchain.ipynb) on how to Evaluate your Langchain Application with Opik! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "Opik is an open-source tool with a growing community. Head over to our [github repo](https://github.com/comet-ml/opik?tab=readme-ov-file) to get the most up to date information on what's the latest and greatest with Opik!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
