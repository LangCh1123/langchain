{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1173893c4f0ea56",
   "metadata": {
    "collapsed": false,
    "id": "e1173893c4f0ea56",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Long Context Embedding aka Late Chunking\n",
    "\n",
    "Based on the [Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models](https://arxiv.org/abs/2409.04701) paper.\n",
    "This paper proposes a technique to add the document-level context information to the individual chunks.\n",
    "\n",
    "Code based  on the [Late Chunking](https://jina.ai/news/late-chunking-in-long-context-embedding-models) blog post.\n",
    "\n",
    "All credits to [Jina](https://jina.ai/)!\n",
    "\n",
    "This notebooks explains how the `Long Context Embedding` can be implemented with `LangChain`.\n",
    "\n",
    "**Notes:**\n",
    "- [Opionated!] This notebook uses the `Long Context Embedding` term which is more suitable than `Late Chunking` term.\n",
    "- `Text chunking` term used in the paper was replaced with the `text splitting` term that is used in LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e062281-d90f-487b-9271-7ae6c0a8416e",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a920f-cde0-4035-9834-49b087aab5cc",
   "metadata": {
    "id": "d02a920f-cde0-4035-9834-49b087aab5cc",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a8fbc1e477db48",
   "metadata": {
    "collapsed": false,
    "id": "58a8fbc1e477db48",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Load a model which we want to use for the embedding. We choose `jinaai/jina-embeddings-v2-base-en` but any other model which supports mean pooling is possible. Models with a large maximum context-length are preferred for the long context embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1380abf7acde9517",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-10-11T18:15:09.358388Z",
     "iopub.status.busy": "2024-10-11T18:15:09.357996Z",
     "iopub.status.idle": "2024-10-11T18:15:11.332039Z",
     "shell.execute_reply": "2024-10-11T18:15:11.328537Z",
     "shell.execute_reply.started": "2024-10-11T18:15:09.358354Z"
    },
    "id": "1380abf7acde9517",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ef33f63d-535b-44ec-c1b0-5c06815c716a"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd13de6b-a741-420c-b390-115397cfb036",
   "metadata": {},
   "source": [
    "An illustration of the lost context problem. Here is a Wikipedia article about `Berlin`.\n",
    "One can see that phrases like “its” and “the city” reference “Berlin,” which is mentioned\n",
    "only in the first sentence. This makes it harder for the embedding model to link these references to\n",
    "the correct entity, thereby producing a lower-quality vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae0f5f9-e737-4af6-a0f7-bd6ef2eda629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T18:15:11.334394Z",
     "iopub.status.busy": "2024-10-11T18:15:11.334280Z",
     "iopub.status.idle": "2024-10-11T18:15:11.338334Z",
     "shell.execute_reply": "2024-10-11T18:15:11.337715Z",
     "shell.execute_reply.started": "2024-10-11T18:15:11.334385Z"
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"Berlin is the capital and largest city of Germany, both by area and by population. Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits. The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc0c1162797ffb0",
   "metadata": {
    "collapsed": false,
    "id": "2cc0c1162797ffb0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Text splitting\n",
    "\n",
    "### Naive text splitting\n",
    "\n",
    "We split text by the sentence separators. We use `.` character as a separator.\n",
    "We save separators as a part of chunks.\n",
    "\n",
    "In real life, we would use more robust and soficticated text splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e61a2fc-7f64-4bc1-84d5-39af84321c6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T18:15:11.339068Z",
     "iopub.status.busy": "2024-10-11T18:15:11.338942Z",
     "iopub.status.idle": "2024-10-11T18:15:11.342570Z",
     "shell.execute_reply": "2024-10-11T18:15:11.342293Z",
     "shell.execute_reply.started": "2024-10-11T18:15:11.339058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Berlin is the capital and largest city of Germany, both by area and by population', \"Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits\", 'The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.']\n"
     ]
    }
   ],
   "source": [
    "chunks = input_text.split(\". \")\n",
    "\n",
    "# take care of the separator at the end of the text:\n",
    "naive_chunks = [(chunk + \".\").replace(\"..\", \".\") for chunk in chunks]\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e41e5-0ffc-4508-bcb3-90c4654ec566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-10T22:10:33.798640Z",
     "iopub.status.busy": "2024-10-10T22:10:33.798426Z",
     "iopub.status.idle": "2024-10-10T22:10:33.801248Z",
     "shell.execute_reply": "2024-10-10T22:10:33.800833Z",
     "shell.execute_reply.started": "2024-10-10T22:10:33.798626Z"
    }
   },
   "source": [
    "## Traditional chunk embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbaa9935-f1ed-466a-a298-5e3f498e9c7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T18:15:11.343380Z",
     "iopub.status.busy": "2024-10-11T18:15:11.343255Z",
     "iopub.status.idle": "2024-10-11T18:15:11.444158Z",
     "shell.execute_reply": "2024-10-11T18:15:11.443785Z",
     "shell.execute_reply.started": "2024-10-11T18:15:11.343372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3, Embedding dimensions: 768\n",
      "Embedding sample[:10]: [-0.7992611  -0.67268556  0.9821002   0.28078204 -0.08286519  0.0186394\n",
      "  0.14283076  0.13469528  0.14336902 -0.04381512]...\n"
     ]
    }
   ],
   "source": [
    "traditional_embeddings = model.encode(chunks)\n",
    "print(\n",
    "    f\"Number of chunks: {len(traditional_embeddings)}, Embedding dimensions: {len(traditional_embeddings[0])}\"\n",
    ")\n",
    "print(f\"Embedding sample[:10]: {traditional_embeddings[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b55483d-a0f2-4f20-af3d-30095fb99b91",
   "metadata": {},
   "source": [
    "## Long Context chunk embedding\n",
    "\n",
    "Pseudo-code:\n",
    "- For each chunk in text:\n",
    "  - For each individual word in chunk:\n",
    "    - Get word_context = (all text before the word) + the word\n",
    "    - Calculate embedding for word as for the word_context # Now this embedding includes all previous text as the word context\n",
    "  - Calculate chunk embedding as the average of the chunk word embeddings.\n",
    " \n",
    "Note:\n",
    "- We do not limit of the word context, we use all text that preceed the word. The paper mentioned that we could use this limit as a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfc1385a-e3d8-48e8-ac6c-36984fa9472e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T18:15:11.444762Z",
     "iopub.status.busy": "2024-10-11T18:15:11.444636Z",
     "iopub.status.idle": "2024-10-11T18:15:12.901738Z",
     "shell.execute_reply": "2024-10-11T18:15:12.901287Z",
     "shell.execute_reply.started": "2024-10-11T18:15:11.444751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 3, long_context_embeddings lenght: 3, Embedding dimensions: 768\n",
      "Embedding sample[:10]: [-0.6337145  -0.67458665  0.84166676  0.31604436 -0.21064001  0.24656577\n",
      "  0.12800963  0.03115163  0.15378839  0.05417303]...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_chunk_long_context_embeddings(chunks, model):\n",
    "    chunks_embeddings = []\n",
    "    left_words = []\n",
    "    for chunk in chunks:\n",
    "        chunk_subchunks = []\n",
    "        for word in chunk.strip().split(\" \"):\n",
    "            left_words.append(word)\n",
    "            chunk_subchunks.append(\" \".join(left_words))\n",
    "        chunk_embeddings = model.encode(chunk_subchunks)\n",
    "        chunk_embeddings_avg = np.mean(chunk_embeddings, axis=0)\n",
    "        chunks_embeddings.append(chunk_embeddings_avg)\n",
    "    return chunks_embeddings\n",
    "\n",
    "\n",
    "long_context_embeddings = calc_chunk_long_context_embeddings(chunks, model)\n",
    "print(\n",
    "    f\"Number of chunks: {len(chunks)}, long_context_embeddings lenght: {len(long_context_embeddings)}, Embedding dimensions: {len(long_context_embeddings[0])}\"\n",
    ")\n",
    "print(f\"Embedding sample[:10]: {long_context_embeddings[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b44517b-fc82-4073-b361-10f356eda693",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Finally, we compare the similarity of the word \"Berlin\" with the chunks. The similarity should be higher for the long context method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dce9ef4-410a-4d61-9077-0c32175e4638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-11T18:15:12.902468Z",
     "iopub.status.busy": "2024-10-11T18:15:12.902315Z",
     "iopub.status.idle": "2024-10-11T18:15:12.928208Z",
     "shell.execute_reply": "2024-10-11T18:15:12.926880Z",
     "shell.execute_reply.started": "2024-10-11T18:15:12.902456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity of 'Berlin' vs 'Berlin is the capital and largest city of Germany, both by area and by population':\n",
      "  long context embedding: 0.900\n",
      "  traditional embedding: 0.838\n",
      "\n",
      "Similarity of 'Berlin' vs 'Its more than 3.85 million inhabitants make it the European Union's most populous city, as measured by population within city limits':\n",
      "  long context embedding: 0.862\n",
      "  traditional embedding: 0.704\n",
      "\n",
      "Similarity of 'Berlin' vs 'The city is also one of the states of Germany, and is the third smallest state in the country in terms of area.':\n",
      "  long context embedding: 0.859\n",
      "  traditional embedding: 0.753\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cos_sim(x, y):\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "\n",
    "berlin_embedding = model.encode(\"Berlin\")\n",
    "\n",
    "for chunk, long_context_embedding, traditional_embedding in zip(\n",
    "    chunks, long_context_embeddings, traditional_embeddings\n",
    "):\n",
    "    print()\n",
    "    print(f\"Similarity of 'Berlin' vs '{chunk}':\")\n",
    "    print(\n",
    "        f\"  long context embedding: {cos_sim(berlin_embedding, long_context_embedding):.3f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  traditional embedding: {cos_sim(berlin_embedding, traditional_embedding):.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7277a-14f6-4bc8-8bfe-87aae46b6622",
   "metadata": {},
   "source": [
    "As you can see the long context method helps in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13650a7-54c6-4cb9-8705-458154f2fa41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
